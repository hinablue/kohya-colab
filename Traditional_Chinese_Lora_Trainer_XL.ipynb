{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# 🌟 XL 訓練器 by Hollowstrawberry\n",
        "\n",
        "❗ **推薦使用 Colab Premium** 理想情況下，您可以將運行時間變更為 A100 並使用最大批次大小。\n",
        "但是，如果您載入擴散器模型，您仍然可以免費訓練，只是需要更長的時間。\n",
        "\n",
        "\n",
        "基於 [Kohya-ss](https://github.com/kohya-ss/sd-scripts) 與 [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) 來完成此工具，感謝。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### ⭕ 免則聲明\n",
        "本文件是用於研究機器學習等前端技術為目的。\n",
        "請閱讀以下文件說明 [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) 與 [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|🇬🇧 English|🇪🇸 Spanish|🇹🇼 繁體中文|\n",
        "|:--|:-:|:-:|:-:|:-:|\n",
        "| 🏠 **首頁** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n",
        "| 📊 **資料集製作器** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) | |\n",
        "| ⭐ **Lora 訓練器** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) | [![在 Colab 開啟](https://raw.githubusercontent.com/hinablue/kohya-colab/main/assets/colab-badge-tw.svg)](https://colab.research.google.com/github/hinablue/kohya-colab/blob/main/Traditional_Chinese_Lora_Trainer.ipynb) |\n",
        "| ⭐ **XL Lora 訓練器** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![在 Colab 開啟](https://raw.githubusercontent.com/hinablue/kohya-colab/main/assets/colab-badge-tw.svg)](https://colab.research.google.com/github/hinablue/kohya-colab/blob/main/Traditional_Chinese_Lora_Trainer_XL.ipynb) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OglZzI_ujZq-",
        "outputId": "e8af5902-6785-4d01-d6f6-9c1559e3a6c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "from time import time\n",
        "from huggingface_hub import HfFileSystem\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "\n",
        "COLAB = True\n",
        "SOURCE = \"https://github.com/qaneel/kohya-trainer\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "try:\n",
        "  LOWRAM = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "except:\n",
        "  LOWRAM = False\n",
        "\n",
        "#@title ## 🚩 Start Here\n",
        "\n",
        "#@markdown ### ▶️ 設定\n",
        "#@markdown 你的專案名稱必須和包含圖片的資料夾名稱相同。不允許使用空格。\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "#@markdown 資料夾結構不重要，只是為了方便。請確保每次都選擇相同的結構。我傾向使用以專案模式的方式。\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "#@markdown 選擇並下載訓練所需要使用的模型。這些選項應該會產生乾淨且一致的結果。你也可以選擇自己的模型，只要貼上下載連結即可。\n",
        "training_model = \"Pony Diffusion V6 XL\" #@param [\"Pony Diffusion V6 XL\", \"Animagine XL V3\", \"Stable Diffusion XL 1.0 base\"]\n",
        "optional_custom_training_model_url = \"\" #@param {type:\"string\"}\n",
        "#@markdown 自訂模型的下載連結。如果你沒有自訂模型，請留空。\n",
        "load_diffusers = True #@param {type:\"boolean\"}\n",
        "\n",
        "if optional_custom_training_model_url:\n",
        "  model_url = optional_custom_training_model_url\n",
        "elif \"Pony\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stablediffusionapi/pony-diffusion-v6-xl/\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/290640\"\n",
        "  model_file = \"/content/ponyDiffusionV6XL.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/293564\"\n",
        "  model_file = \"/content/animagineXLV3.safetensors\"\n",
        "else:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "\n",
        "if load_diffusers:\n",
        "  vae_file= \"stabilityai/sdxl-vae\"\n",
        "else:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "  vae_file = \"/content/sdxl_vae.safetensors\"\n",
        "\n",
        "\n",
        "#@markdown ### ▶️ 加工處理\n",
        "resolution = 1024 #param {type:\"slider\", min:768, max:1536, step:128}\n",
        "caption_extension = \".txt\" #param {type:\"string\"}\n",
        "#@markdown 這個選項會在訓練時自動翻轉圖片，不會增加訓練時間，但是可以讓模型學習更多。如果你的圖片少於 20 張，請務必開啟此選項。\n",
        "#@markdown **如果你在意圖片的對稱性，請關閉此選項。**\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "#@markdown 針對動畫標籤進行洗牌可以提高學習和提示的效果。文字檔案開頭的啟動標籤，不會被洗牌。\n",
        "shuffle_tags = True #@param {type:\"boolean\"}\n",
        "shuffle_caption = shuffle_tags\n",
        "#@markdown 啟動標籤數量。如果你的圖片沒有啟動標籤，請設定為 0。請務必確認你的啟動標籤放在文字檔案的最前面。\n",
        "activation_tags = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### ▶️ 步數\n",
        "#@markdown 你的圖片會在訓練時重複這個次數。我建議你的圖片乘以重複次數介於 200 到 400 之間。\n",
        "num_repeats = 2 #@param {type:\"number\"}\n",
        "#@markdown 選擇你想要訓練的時間。一個好的起點是 10 個輪次（Epoch） 或 2000 個步數（Steps)。\n",
        "#@markdown 每一個輪次（Epoch）的步數等於：你的圖片數量乘以重複次數，除以批次大小。\n",
        "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
        "how_many = 10 #@param {type:\"number\"}\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "#@markdown 儲存每次的輪次（Epochs）可以讓你更好的比較你的 Lora 的進度。\n",
        "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 10 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### ▶️ 學習設定\n",
        "#@markdown 學習率是你的結果最重要的因素。如果你想要訓練更慢，或是你的圖片數量很多，或是你的 dim 和 alpha 很高，請把 UNet 的學習率調到 2e-4 或更低。\n",
        "#@markdown 文字編碼器可以讓你的 Lora 學習概念更好。建議你把文字編碼器的學習率設定為 UNet 的一半或五分之一。如果你在訓練風格，你甚至可以把它設定為 0。\n",
        "unet_lr = 3e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = 6e-5 #@param {type:\"number\"}\n",
        "#@markdown 調度器是指導學習率的演算法。如果你不確定，請選擇 `constant` 並忽略重啟數字（`lr_scheduler_number`）。若使用 `cosine_with_restarts` ，我個人建議使用 3 次重啟數字。\n",
        "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown 在訓練時，用來「熱身」學習率的步數（僅適用於 `constant_with_warmup`）。我建議你把它設定為 5%。\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
        "lr_warmup_steps = 0\n",
        "#@markdown 新功能，可以隨著時間調整損失函數，讓學習更有效率，並且可以用大約一半的輪次（Epochs）完成訓練。參考[論文](https://arxiv.org/abs/2303.09556)建議，使用 5% 的數值。\n",
        "min_snr_gamma = 5.0 #@param {type:\"slider\", min:0.0, max:16.0, step:0.5}\n",
        "\n",
        "#@markdown ### ▶️ 模型結構\n",
        "#@markdown LoRA 是經典的類型，而 LoCon 則是適合風格類型的訓練。在 WebUI 中使用 LyCORIS 需要[這個擴充功能](https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris)。若想要更多資訊，請參考[這裡](https://github.com/KohakuBlueleaf/Lycoris)。\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon\"]\n",
        "\n",
        "#@markdown Below are some recommended XL values for the following settings:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | Regular LoRA | 8 | 4 |   |   |\n",
        "#@markdown | Style LoCon | 16 | 8 | 16 | 8 |\n",
        "\n",
        "#@markdown 更大的 dim 代表更大的 Lora，它可以儲存更多資訊，但是並不是越大越好。建議的 dim 為 8-32，alpha 則是 dim 的一半。\n",
        "network_dim = 8 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "network_alpha = 4 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "#@markdown 以下的數值不會影響 LoRA。它們的作用類似於 dim/alpha，但僅適用於 LyCORIS 的額外學習層。\n",
        "conv_dim = 4 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "conv_alpha = 1 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "conv_compression = False #@param {type:\"boolean\"}\n",
        "\n",
        "network_module = \"lycoris.kohya\" if \"LoCon\" in lora_type else \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### ▶️ 更多設定\n",
        "#@markdown 建議使用 0.15 ~ 0.4 之間的數值，數值越大，訓練生成的結果越接近正規化圖片。\n",
        "prior_loss_weight = 0.1 #@param {type:\"number\"}\n",
        "#@markdown 噪訊偏移可以改善亮度/暗度的處理結果。\n",
        "noise_offset = 0.1 #@param {type:\"number\"}\n",
        "#@markdown 黑魔法，建議值使用 2，若訓練真實人物等模型，可以使用 1。\n",
        "clip_skip = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "#@markdown 最大文字標籤長度，預設為 225。\n",
        "max_token_length = 225 #@param [75,125,225]\n",
        "#@markdown 啟用不放大批次解析度，如果你想要使用 最大/最小 批次解析度，請設定為 `False`。\n",
        "bucket_no_upscale = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **以下設定僅適用於不放大批次解析度為 `False` 時。**\n",
        "\n",
        "#@markdown 若解析度小於 512，建議使用 256，否則建議使用 320。\n",
        "min_bucket_reso = 256 #@param {type:\"number\"}\n",
        "#@markdown 若解析度小於 512，建議使用 1024，否則建議使用 1280。\n",
        "max_bucket_reso = 1024 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown 顏色增強可以改善結果的顏色。如果啟用，將會強制關閉快取潛在變數快取（`Latent Cache = False`）。\n",
        "color_aug = False #@param {type:\"boolean\"}\n",
        "#@markdown 依據臉部中心尺寸擷取圖片，再依此上限、下限決定擷取的範圍。可優化臉部訓練。若訓練包含背景風格可將此數值上限、下限加大。\n",
        "face_crop_aug_range = [1.0, 3.0] #@param {type:\"raw\"}\n",
        "#@markdown Random crop the image, recommend using False.\n",
        "#@markdown 隨機擷取圖片區域，建議使用 `False`，若訓練風格類型則建議開啟。\n",
        "random_crop = False #@param {type:\"boolean\"}\n",
        "#@markdown 增加梯度累積步驟以節省 GPU 記憶體。但會降低訓練速度，並需要更高的學習率。\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ### ▶️ 實驗性功能\n",
        "#@markdown 儲存額外資料，約 1 GB，可以讓你稍後繼續訓練。\n",
        "save_state = False #@param {type:\"boolean\"}\n",
        "#@markdown 如果有儲存的額外資料，則繼續訓練。\n",
        "resume = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ▶️ 訓練設定\n",
        "#@markdown 根據您的 Colab 配置調整這些參數。\n",
        "#@markdown 如果您使用免費方案，則應在此儲存格頂部選擇模型。\n",
        "#@markdown\n",
        "#@markdown 批次大小越大通常速度越快，但會佔用更多記憶體。\n",
        "train_batch_size = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown 我發現 sdpa 和 xformers 之間沒有實質差異。\n",
        "cross_attention = \"sdpa\" #@param [\"sdpa\", \"xformers\"]\n",
        "#@markdown 如果您使用的是 A100，則應啟用 bf16。\n",
        "mixed_precision = \"fp16\" #@param [\"bf16\", \"fp16\"]\n",
        "#@markdown 快取潛在快取緩存，將在每個圖像旁邊添加一個 250KB 文件，但會使用相當少的記憶體。\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "cache_latents_to_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown 以下選項將關閉 shuffle_tags 並停用文字編碼器訓練。\n",
        "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ▶️ 進階設定\n",
        "#@markdown 優化器是用於訓練的演算法。 AdanW8Bit 是預設設定並且效果很好，而 Prodigy 會自動管理學習率，並且可能具有多種優勢，例如由於需要更少的步驟而訓練速度更快，以及對於小型資料集更好地工作。\n",
        "optimizer = \"AdamW8bit\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\"]\n",
        "#@markdown AdamW8bit 推薦使用: `weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
        "#@markdown Prodigy 推薦使用: `decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`\n",
        "optimizer_args = \"weight_decay=0.1 betas=[0.9,0.99]\" #@param {type:\"string\"}\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "#@markdown 如果選擇了 Dadapt 或 Prodigy 並勾選此框，則以下建議值將覆蓋任何先前的設定：<p>\n",
        "#@markdown `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`\n",
        "recommended_values_for_prodigy = True #@param {type:\"boolean\"}\n",
        "\n",
        "if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
        "  if recommended_values_for_prodigy:\n",
        "    unet_lr = 0.75\n",
        "    text_encoder_lr = 0.75\n",
        "    network_alpha = network_dim\n",
        "\n",
        "#@markdown 你可以在這裡寫下你的 Google Drive 中的路徑，以載入現有的 Lora 檔案，以繼續訓練。\n",
        "#@markdown **警告** 這不是一個長時間的訓練階段。每個 epoch 都是從頭開始，並且可能會有更差的結果。\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "if continue_from_lora:\n",
        "  if not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "    continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"📂 連接到 Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  if not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"💥 錯誤：無法讀取既有 Lora 檔案，訓練將無法從此現有 Lora 繼續訓練。\")\n",
        "\n",
        "#@markdown ### ▶️ 準備好了\n",
        "#@markdown 你現在可以執行此儲存格來開始訓練。祝你好運！<p>\n",
        "\n",
        "\n",
        "# 👩‍💻 Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_xl_wrapper.py -q -O train_network_xl_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install torch==2.2.1+cu121 accelerate==0.19.0 transformers==4.30.2 diffusers==0.18.2 bitsandbytes==0.40.0.post4 opencv-python==4.7.0.68 jax==0.4.23 jaxlib==0.4.23\n",
        "  !pip install pytorch-lightning==1.9.0 voluptuous==0.13.1 toml==0.10.2 ftfy==6.1.1 einops==0.6.0 safetensors pygments\n",
        "  !pip install huggingface-hub==0.20.3 invisible-watermark==0.2.0 open-clip-torch==2.20.0 dadaptation==3.1 prodigyopt==1.0 lion-pytorch==0.1.2\n",
        "  !pip install -e .\n",
        "  if cross_attention == \"xformers\":\n",
        "    !pip install -q xformers==0.0.26.dev778\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if LOWRAM:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\n💿 檢查資料集...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"💥 錯誤：請選擇正確的專案名稱。\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datconf[\"datasets\"][0][\"subsets\"] if not (d[\"is_reg\"] and d[\"is_reg\"] == True)}\n",
        "    except:\n",
        "      print(f\"💥 錯誤：你的自訂資料集結構錯誤，請確認資料集結構。\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"💥 錯誤：資料夾 {folder.replace('/content/drive/', '')} 不存在。\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"💥 錯誤：你的 {folder.replace('/content/drive/', '')} 資料夾沒有資料。\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((\".txt\", \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"💥 錯誤：錯誤的資料集結構 \\\"{f}\\\"，中斷執行。\")\n",
        "      return\n",
        "\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"💥 錯誤：錯誤的路徑或 Lora 不存在。範例：/content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"📁\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"📈 讀取 {img} 圖片，重複 {rep} 次，共有 {img*rep} 個步驟。\")\n",
        "  print(f\"📉 訓練批次 {train_batch_size} 除以總步驟數量 {pre_steps_per_epoch}，得到每一輪次處理 {int(steps_per_epoch)} 個步驟。\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"🔮 最大輪次 {max_train_epochs}，大約會有 {total_steps} 個總訓練步驟。\")\n",
        "  else:\n",
        "    print(f\"🔮 總訓練步驟 {total_steps}，並除以 {estimated_epochs} 個輪次。\")\n",
        "\n",
        "  if total_steps > 12000:\n",
        "    print(\"💥 錯誤：你的總訓練步驟過高，可能會造成錯誤。中斷訓練。\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n⭕ 使用自訂設定檔 {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": cross_attention == \"xformers\",\n",
        "        \"sdpa\": cross_attention == \"sdpa\",\n",
        "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma > 0 else None,\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\",\n",
        "        \"cache_latents\": cache_latents,\n",
        "        \"cache_latents_to_disk\": cache_latents_to_drive,\n",
        "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": prior_loss_weight,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\n📄 Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"⭕ Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": bucket_no_upscale,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 4096,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"\\n📄 設定檔已儲存到 {config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file, vae_url, vae_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if load_diffusers:\n",
        "    if 'huggingface.co' in real_model_url:\n",
        "        match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', real_model_url)\n",
        "        if match:\n",
        "            username = match.group(1)\n",
        "            model_name = match.group(2)\n",
        "            model_file = f\"{username}/{model_name}\"\n",
        "            fs = HfFileSystem()\n",
        "            existing_folders = set(fs.ls(model_file, detail=False))\n",
        "            necessary_folders = [ \"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\" ]\n",
        "            if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n",
        "              print(\"🍃 Diffusers model identified.\")\n",
        "              return True\n",
        "    print(\"🍃 無法讀取模型檔案。\")\n",
        "\n",
        "  if not model_file:\n",
        "    if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "      model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "    else:\n",
        "      model_file = \"/content/downloaded_model.safetensors\"\n",
        "      if os.path.exists(model_file):\n",
        "        !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", model_url):\n",
        "    if m.group(2):\n",
        "      model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "    if m := re.search(r\"modelVersionId=([0-9]+)\", model_url):\n",
        "      real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    else:\n",
        "      raise ValueError(\"optional_custom_training_model_url contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if not os.path.exists(vae_file):\n",
        "    !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"重新命名為 {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"📂 Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\n🏭 安裝相依套件...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n✅ 安裝完成，耗時 {int(t1-t0)} 秒。\")\n",
        "  else:\n",
        "    print(\"\\n✅ 相依套件已安裝。\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\n🔄 下載模型...\")\n",
        "    if not download_model():\n",
        "      print(\"\\n💥 錯誤：您選擇的模型無效或已損壞，或無法下載。您可以使用 civitai 或 Huggingface 鏈接，或任何直接下載鏈接。\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\n🔄 模型已下載。\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n⭐ 啟動訓練...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --quiet --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_xl_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ✅ 完成 [從 Google 雲端硬碟下載您的 Lora](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                     \"### 會有幾個文件，你應該嘗試最新版本（旁邊數字最大的文件）\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      },
      "source": [
        "## *️⃣ 擴充功能\n",
        "\n",
        "在開始訓練之前，你可以執行以下的功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4916Eu1tb9"
      },
      "source": [
        "### 📚 複數資料夾的資料集\n",
        "以下的樣版允許你在資料集中，定義複數的資料夾。你需要包含每個資料集的檔案路徑，且指定每個資料集的重複次數。你可以直接複製 `[[datasets.subsets]]` 區塊，簡單的增加你的資料集。\n",
        "\n",
        "當你使用這個設定，在原本訓練中的重複次數的設定將會被忽略，且依據專案的資料集設定也會被忽略。\n",
        "\n",
        "你可以加入 `ìs_reg = true` 將某一個資料集設定為正規化（regularization）資料。\n",
        "你也可以設定各種不同的參數，例如 `keep_tokens`, `flip_aug` 等等。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 804,
          "status": "ok",
          "timestamp": 1708481414906,
          "user": {
            "displayName": "Samuel",
            "userId": "11002898838993959229"
          },
          "user_tz": 180
        },
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 3\n",
        "is_reg = false\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "is_reg = false\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/reg_images\"\n",
        "num_repeats = 1\n",
        "is_reg = true\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WDjkp4scvPgE"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 📂 Unzip dataset\n",
        "#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\n",
        "zip = \"/content/drive/MyDrive/my_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"📂 Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"✅ 完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKWlpsG0jrX3"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 🔢 Count datasets\n",
        "#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\n",
        "folder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"📂 連接到 Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"📁{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcTjh07x90Ro"
      },
      "source": [
        "# 📈 繪製訓練結果\n",
        "您可以在運行訓練器後執行此操作。除非您知道自己在做什麼，否則您不需要這個。\n",
        "下面的第一個儲存格可能無法載入您的所有日誌。繼續嘗試第二個單元格，直到加載所有資料。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TRI3eX90Rp"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rM5SLq990Rp"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
