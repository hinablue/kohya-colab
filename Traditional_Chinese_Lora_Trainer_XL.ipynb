{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# ğŸŒŸ XL è¨“ç·´å™¨ by Hollowstrawberry\n",
        "\n",
        "â— **æ¨è–¦ä½¿ç”¨ Colab Premium** ç†æƒ³æƒ…æ³ä¸‹ï¼Œæ‚¨å¯ä»¥å°‡é‹è¡Œæ™‚é–“è®Šæ›´ç‚º A100 ä¸¦ä½¿ç”¨æœ€å¤§æ‰¹æ¬¡å¤§å°ã€‚\n",
        "ä½†æ˜¯ï¼Œå¦‚æœæ‚¨è¼‰å…¥æ“´æ•£å™¨æ¨¡å‹ï¼Œæ‚¨ä»ç„¶å¯ä»¥å…è²»è¨“ç·´ï¼Œåªæ˜¯éœ€è¦æ›´é•·çš„æ™‚é–“ã€‚\n",
        "\n",
        "\n",
        "åŸºæ–¼ [Kohya-ss](https://github.com/kohya-ss/sd-scripts) èˆ‡ [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) ä¾†å®Œæˆæ­¤å·¥å…·ï¼Œæ„Ÿè¬ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### â­• å…å‰‡è²æ˜\n",
        "æœ¬æ–‡ä»¶æ˜¯ç”¨æ–¼ç ”ç©¶æ©Ÿå™¨å­¸ç¿’ç­‰å‰ç«¯æŠ€è¡“ç‚ºç›®çš„ã€‚\n",
        "è«‹é–±è®€ä»¥ä¸‹æ–‡ä»¶èªªæ˜ [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) èˆ‡ [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|ğŸ‡¬ğŸ‡§ English|ğŸ‡ªğŸ‡¸ Spanish|ğŸ‡¹ğŸ‡¼ ç¹é«”ä¸­æ–‡|\n",
        "|:--|:-:|:-:|:-:|:-:|\n",
        "| ğŸ  **é¦–é ** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n",
        "| ğŸ“Š **è³‡æ–™é›†è£½ä½œå™¨** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) | |\n",
        "| â­ **Lora è¨“ç·´å™¨** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) | [![åœ¨ Colab é–‹å•Ÿ](https://raw.githubusercontent.com/hinablue/kohya-colab/main/assets/colab-badge-tw.svg)](https://colab.research.google.com/github/hinablue/kohya-colab/blob/main/Traditional_Chinese_Lora_Trainer.ipynb) |\n",
        "| â­ **XL Lora è¨“ç·´å™¨** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![åœ¨ Colab é–‹å•Ÿ](https://raw.githubusercontent.com/hinablue/kohya-colab/main/assets/colab-badge-tw.svg)](https://colab.research.google.com/github/hinablue/kohya-colab/blob/main/Traditional_Chinese_Lora_Trainer_XL.ipynb) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OglZzI_ujZq-",
        "outputId": "e8af5902-6785-4d01-d6f6-9c1559e3a6c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "from time import time\n",
        "from huggingface_hub import HfFileSystem\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "\n",
        "COLAB = True\n",
        "SOURCE = \"https://github.com/qaneel/kohya-trainer\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "try:\n",
        "  LOWRAM = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "except:\n",
        "  LOWRAM = False\n",
        "\n",
        "#@title ## ğŸš© Start Here\n",
        "\n",
        "#@markdown ### â–¶ï¸ è¨­å®š\n",
        "#@markdown ä½ çš„å°ˆæ¡ˆåç¨±å¿…é ˆå’ŒåŒ…å«åœ–ç‰‡çš„è³‡æ–™å¤¾åç¨±ç›¸åŒã€‚ä¸å…è¨±ä½¿ç”¨ç©ºæ ¼ã€‚\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "#@markdown è³‡æ–™å¤¾çµæ§‹ä¸é‡è¦ï¼Œåªæ˜¯ç‚ºäº†æ–¹ä¾¿ã€‚è«‹ç¢ºä¿æ¯æ¬¡éƒ½é¸æ“‡ç›¸åŒçš„çµæ§‹ã€‚æˆ‘å‚¾å‘ä½¿ç”¨ä»¥å°ˆæ¡ˆæ¨¡å¼çš„æ–¹å¼ã€‚\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "#@markdown é¸æ“‡ä¸¦ä¸‹è¼‰è¨“ç·´æ‰€éœ€è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚é€™äº›é¸é …æ‡‰è©²æœƒç”¢ç”Ÿä¹¾æ·¨ä¸”ä¸€è‡´çš„çµæœã€‚ä½ ä¹Ÿå¯ä»¥é¸æ“‡è‡ªå·±çš„æ¨¡å‹ï¼Œåªè¦è²¼ä¸Šä¸‹è¼‰é€£çµå³å¯ã€‚\n",
        "training_model = \"Pony Diffusion V6 XL\" #@param [\"Pony Diffusion V6 XL\", \"Animagine XL V3\", \"Stable Diffusion XL 1.0 base\"]\n",
        "optional_custom_training_model_url = \"\" #@param {type:\"string\"}\n",
        "#@markdown è‡ªè¨‚æ¨¡å‹çš„ä¸‹è¼‰é€£çµã€‚å¦‚æœä½ æ²’æœ‰è‡ªè¨‚æ¨¡å‹ï¼Œè«‹ç•™ç©ºã€‚\n",
        "load_diffusers = True #@param {type:\"boolean\"}\n",
        "\n",
        "if optional_custom_training_model_url:\n",
        "  model_url = optional_custom_training_model_url\n",
        "elif \"Pony\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stablediffusionapi/pony-diffusion-v6-xl/\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/290640\"\n",
        "  model_file = \"/content/ponyDiffusionV6XL.safetensors\"\n",
        "elif \"Animagine\" in training_model:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n",
        "  else:\n",
        "    model_url = \"https://civitai.com/api/download/models/293564\"\n",
        "  model_file = \"/content/animagineXLV3.safetensors\"\n",
        "else:\n",
        "  if load_diffusers:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\"\n",
        "  else:\n",
        "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
        "\n",
        "if load_diffusers:\n",
        "  vae_file= \"stabilityai/sdxl-vae\"\n",
        "else:\n",
        "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
        "  vae_file = \"/content/sdxl_vae.safetensors\"\n",
        "\n",
        "\n",
        "#@markdown ### â–¶ï¸ åŠ å·¥è™•ç†\n",
        "resolution = 1024 #param {type:\"slider\", min:768, max:1536, step:128}\n",
        "caption_extension = \".txt\" #param {type:\"string\"}\n",
        "#@markdown é€™å€‹é¸é …æœƒåœ¨è¨“ç·´æ™‚è‡ªå‹•ç¿»è½‰åœ–ç‰‡ï¼Œä¸æœƒå¢åŠ è¨“ç·´æ™‚é–“ï¼Œä½†æ˜¯å¯ä»¥è®“æ¨¡å‹å­¸ç¿’æ›´å¤šã€‚å¦‚æœä½ çš„åœ–ç‰‡å°‘æ–¼ 20 å¼µï¼Œè«‹å‹™å¿…é–‹å•Ÿæ­¤é¸é …ã€‚\n",
        "#@markdown **å¦‚æœä½ åœ¨æ„åœ–ç‰‡çš„å°ç¨±æ€§ï¼Œè«‹é—œé–‰æ­¤é¸é …ã€‚**\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "#@markdown é‡å°å‹•ç•«æ¨™ç±¤é€²è¡Œæ´—ç‰Œå¯ä»¥æé«˜å­¸ç¿’å’Œæç¤ºçš„æ•ˆæœã€‚æ–‡å­—æª”æ¡ˆé–‹é ­çš„å•Ÿå‹•æ¨™ç±¤ï¼Œä¸æœƒè¢«æ´—ç‰Œã€‚\n",
        "shuffle_tags = True #@param {type:\"boolean\"}\n",
        "shuffle_caption = shuffle_tags\n",
        "#@markdown å•Ÿå‹•æ¨™ç±¤æ•¸é‡ã€‚å¦‚æœä½ çš„åœ–ç‰‡æ²’æœ‰å•Ÿå‹•æ¨™ç±¤ï¼Œè«‹è¨­å®šç‚º 0ã€‚è«‹å‹™å¿…ç¢ºèªä½ çš„å•Ÿå‹•æ¨™ç±¤æ”¾åœ¨æ–‡å­—æª”æ¡ˆçš„æœ€å‰é¢ã€‚\n",
        "activation_tags = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "#@markdown ### â–¶ï¸ æ­¥æ•¸\n",
        "#@markdown ä½ çš„åœ–ç‰‡æœƒåœ¨è¨“ç·´æ™‚é‡è¤‡é€™å€‹æ¬¡æ•¸ã€‚æˆ‘å»ºè­°ä½ çš„åœ–ç‰‡ä¹˜ä»¥é‡è¤‡æ¬¡æ•¸ä»‹æ–¼ 200 åˆ° 400 ä¹‹é–“ã€‚\n",
        "num_repeats = 2 #@param {type:\"number\"}\n",
        "#@markdown é¸æ“‡ä½ æƒ³è¦è¨“ç·´çš„æ™‚é–“ã€‚ä¸€å€‹å¥½çš„èµ·é»æ˜¯ 10 å€‹è¼ªæ¬¡ï¼ˆEpochï¼‰ æˆ– 2000 å€‹æ­¥æ•¸ï¼ˆSteps)ã€‚\n",
        "#@markdown æ¯ä¸€å€‹è¼ªæ¬¡ï¼ˆEpochï¼‰çš„æ­¥æ•¸ç­‰æ–¼ï¼šä½ çš„åœ–ç‰‡æ•¸é‡ä¹˜ä»¥é‡è¤‡æ¬¡æ•¸ï¼Œé™¤ä»¥æ‰¹æ¬¡å¤§å°ã€‚\n",
        "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
        "how_many = 10 #@param {type:\"number\"}\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "#@markdown å„²å­˜æ¯æ¬¡çš„è¼ªæ¬¡ï¼ˆEpochsï¼‰å¯ä»¥è®“ä½ æ›´å¥½çš„æ¯”è¼ƒä½ çš„ Lora çš„é€²åº¦ã€‚\n",
        "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 10 #@param {type:\"number\"}\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "#@markdown ### â–¶ï¸ å­¸ç¿’è¨­å®š\n",
        "#@markdown å­¸ç¿’ç‡æ˜¯ä½ çš„çµæœæœ€é‡è¦çš„å› ç´ ã€‚å¦‚æœä½ æƒ³è¦è¨“ç·´æ›´æ…¢ï¼Œæˆ–æ˜¯ä½ çš„åœ–ç‰‡æ•¸é‡å¾ˆå¤šï¼Œæˆ–æ˜¯ä½ çš„ dim å’Œ alpha å¾ˆé«˜ï¼Œè«‹æŠŠ UNet çš„å­¸ç¿’ç‡èª¿åˆ° 2e-4 æˆ–æ›´ä½ã€‚\n",
        "#@markdown æ–‡å­—ç·¨ç¢¼å™¨å¯ä»¥è®“ä½ çš„ Lora å­¸ç¿’æ¦‚å¿µæ›´å¥½ã€‚å»ºè­°ä½ æŠŠæ–‡å­—ç·¨ç¢¼å™¨çš„å­¸ç¿’ç‡è¨­å®šç‚º UNet çš„ä¸€åŠæˆ–äº”åˆ†ä¹‹ä¸€ã€‚å¦‚æœä½ åœ¨è¨“ç·´é¢¨æ ¼ï¼Œä½ ç”šè‡³å¯ä»¥æŠŠå®ƒè¨­å®šç‚º 0ã€‚\n",
        "unet_lr = 3e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = 6e-5 #@param {type:\"number\"}\n",
        "#@markdown èª¿åº¦å™¨æ˜¯æŒ‡å°å­¸ç¿’ç‡çš„æ¼”ç®—æ³•ã€‚å¦‚æœä½ ä¸ç¢ºå®šï¼Œè«‹é¸æ“‡ `constant` ä¸¦å¿½ç•¥é‡å•Ÿæ•¸å­—ï¼ˆ`lr_scheduler_number`ï¼‰ã€‚è‹¥ä½¿ç”¨ `cosine_with_restarts` ï¼Œæˆ‘å€‹äººå»ºè­°ä½¿ç”¨ 3 æ¬¡é‡å•Ÿæ•¸å­—ã€‚\n",
        "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown åœ¨è¨“ç·´æ™‚ï¼Œç”¨ä¾†ã€Œç†±èº«ã€å­¸ç¿’ç‡çš„æ­¥æ•¸ï¼ˆåƒ…é©ç”¨æ–¼ `constant_with_warmup`ï¼‰ã€‚æˆ‘å»ºè­°ä½ æŠŠå®ƒè¨­å®šç‚º 5%ã€‚\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
        "lr_warmup_steps = 0\n",
        "#@markdown æ–°åŠŸèƒ½ï¼Œå¯ä»¥éš¨è‘—æ™‚é–“èª¿æ•´æå¤±å‡½æ•¸ï¼Œè®“å­¸ç¿’æ›´æœ‰æ•ˆç‡ï¼Œä¸¦ä¸”å¯ä»¥ç”¨å¤§ç´„ä¸€åŠçš„è¼ªæ¬¡ï¼ˆEpochsï¼‰å®Œæˆè¨“ç·´ã€‚åƒè€ƒ[è«–æ–‡](https://arxiv.org/abs/2303.09556)å»ºè­°ï¼Œä½¿ç”¨ 5% çš„æ•¸å€¼ã€‚\n",
        "min_snr_gamma = 5.0 #@param {type:\"slider\", min:0.0, max:16.0, step:0.5}\n",
        "\n",
        "#@markdown ### â–¶ï¸ æ¨¡å‹çµæ§‹\n",
        "#@markdown LoRA æ˜¯ç¶“å…¸çš„é¡å‹ï¼Œè€Œ LoCon å‰‡æ˜¯é©åˆé¢¨æ ¼é¡å‹çš„è¨“ç·´ã€‚åœ¨ WebUI ä¸­ä½¿ç”¨ LyCORIS éœ€è¦[é€™å€‹æ“´å……åŠŸèƒ½](https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris)ã€‚è‹¥æƒ³è¦æ›´å¤šè³‡è¨Šï¼Œè«‹åƒè€ƒ[é€™è£¡](https://github.com/KohakuBlueleaf/Lycoris)ã€‚\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon\"]\n",
        "\n",
        "#@markdown Below are some recommended XL values for the following settings:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | Regular LoRA | 8 | 4 |   |   |\n",
        "#@markdown | Style LoCon | 16 | 8 | 16 | 8 |\n",
        "\n",
        "#@markdown æ›´å¤§çš„ dim ä»£è¡¨æ›´å¤§çš„ Loraï¼Œå®ƒå¯ä»¥å„²å­˜æ›´å¤šè³‡è¨Šï¼Œä½†æ˜¯ä¸¦ä¸æ˜¯è¶Šå¤§è¶Šå¥½ã€‚å»ºè­°çš„ dim ç‚º 8-32ï¼Œalpha å‰‡æ˜¯ dim çš„ä¸€åŠã€‚\n",
        "network_dim = 8 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "network_alpha = 4 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "#@markdown ä»¥ä¸‹çš„æ•¸å€¼ä¸æœƒå½±éŸ¿ LoRAã€‚å®ƒå€‘çš„ä½œç”¨é¡ä¼¼æ–¼ dim/alphaï¼Œä½†åƒ…é©ç”¨æ–¼ LyCORIS çš„é¡å¤–å­¸ç¿’å±¤ã€‚\n",
        "conv_dim = 4 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "conv_alpha = 1 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "conv_compression = False #@param {type:\"boolean\"}\n",
        "\n",
        "network_module = \"lycoris.kohya\" if \"LoCon\" in lora_type else \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### â–¶ï¸ æ›´å¤šè¨­å®š\n",
        "#@markdown å»ºè­°ä½¿ç”¨ 0.15 ~ 0.4 ä¹‹é–“çš„æ•¸å€¼ï¼Œæ•¸å€¼è¶Šå¤§ï¼Œè¨“ç·´ç”Ÿæˆçš„çµæœè¶Šæ¥è¿‘æ­£è¦åŒ–åœ–ç‰‡ã€‚\n",
        "prior_loss_weight = 0.1 #@param {type:\"number\"}\n",
        "#@markdown å™ªè¨Šåç§»å¯ä»¥æ”¹å–„äº®åº¦/æš—åº¦çš„è™•ç†çµæœã€‚\n",
        "noise_offset = 0.1 #@param {type:\"number\"}\n",
        "#@markdown é»‘é­”æ³•ï¼Œå»ºè­°å€¼ä½¿ç”¨ 2ï¼Œè‹¥è¨“ç·´çœŸå¯¦äººç‰©ç­‰æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ 1ã€‚\n",
        "clip_skip = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "#@markdown æœ€å¤§æ–‡å­—æ¨™ç±¤é•·åº¦ï¼Œé è¨­ç‚º 225ã€‚\n",
        "max_token_length = 225 #@param [75,125,225]\n",
        "#@markdown å•Ÿç”¨ä¸æ”¾å¤§æ‰¹æ¬¡è§£æåº¦ï¼Œå¦‚æœä½ æƒ³è¦ä½¿ç”¨ æœ€å¤§/æœ€å° æ‰¹æ¬¡è§£æåº¦ï¼Œè«‹è¨­å®šç‚º `False`ã€‚\n",
        "bucket_no_upscale = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **ä»¥ä¸‹è¨­å®šåƒ…é©ç”¨æ–¼ä¸æ”¾å¤§æ‰¹æ¬¡è§£æåº¦ç‚º `False` æ™‚ã€‚**\n",
        "\n",
        "#@markdown è‹¥è§£æåº¦å°æ–¼ 512ï¼Œå»ºè­°ä½¿ç”¨ 256ï¼Œå¦å‰‡å»ºè­°ä½¿ç”¨ 320ã€‚\n",
        "min_bucket_reso = 256 #@param {type:\"number\"}\n",
        "#@markdown è‹¥è§£æåº¦å°æ–¼ 512ï¼Œå»ºè­°ä½¿ç”¨ 1024ï¼Œå¦å‰‡å»ºè­°ä½¿ç”¨ 1280ã€‚\n",
        "max_bucket_reso = 1024 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown é¡è‰²å¢å¼·å¯ä»¥æ”¹å–„çµæœçš„é¡è‰²ã€‚å¦‚æœå•Ÿç”¨ï¼Œå°‡æœƒå¼·åˆ¶é—œé–‰å¿«å–æ½›åœ¨è®Šæ•¸å¿«å–ï¼ˆ`Latent Cache = False`ï¼‰ã€‚\n",
        "color_aug = False #@param {type:\"boolean\"}\n",
        "#@markdown ä¾æ“šè‡‰éƒ¨ä¸­å¿ƒå°ºå¯¸æ“·å–åœ–ç‰‡ï¼Œå†ä¾æ­¤ä¸Šé™ã€ä¸‹é™æ±ºå®šæ“·å–çš„ç¯„åœã€‚å¯å„ªåŒ–è‡‰éƒ¨è¨“ç·´ã€‚è‹¥è¨“ç·´åŒ…å«èƒŒæ™¯é¢¨æ ¼å¯å°‡æ­¤æ•¸å€¼ä¸Šé™ã€ä¸‹é™åŠ å¤§ã€‚\n",
        "face_crop_aug_range = [1.0, 3.0] #@param {type:\"raw\"}\n",
        "#@markdown Random crop the image, recommend using False.\n",
        "#@markdown éš¨æ©Ÿæ“·å–åœ–ç‰‡å€åŸŸï¼Œå»ºè­°ä½¿ç”¨ `False`ï¼Œè‹¥è¨“ç·´é¢¨æ ¼é¡å‹å‰‡å»ºè­°é–‹å•Ÿã€‚\n",
        "random_crop = False #@param {type:\"boolean\"}\n",
        "#@markdown å¢åŠ æ¢¯åº¦ç´¯ç©æ­¥é©Ÿä»¥ç¯€çœ GPU è¨˜æ†¶é«”ã€‚ä½†æœƒé™ä½è¨“ç·´é€Ÿåº¦ï¼Œä¸¦éœ€è¦æ›´é«˜çš„å­¸ç¿’ç‡ã€‚\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ### â–¶ï¸ å¯¦é©—æ€§åŠŸèƒ½\n",
        "#@markdown å„²å­˜é¡å¤–è³‡æ–™ï¼Œç´„ 1 GBï¼Œå¯ä»¥è®“ä½ ç¨å¾Œç¹¼çºŒè¨“ç·´ã€‚\n",
        "save_state = False #@param {type:\"boolean\"}\n",
        "#@markdown å¦‚æœæœ‰å„²å­˜çš„é¡å¤–è³‡æ–™ï¼Œå‰‡ç¹¼çºŒè¨“ç·´ã€‚\n",
        "resume = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### â–¶ï¸ è¨“ç·´è¨­å®š\n",
        "#@markdown æ ¹æ“šæ‚¨çš„ Colab é…ç½®èª¿æ•´é€™äº›åƒæ•¸ã€‚\n",
        "#@markdown å¦‚æœæ‚¨ä½¿ç”¨å…è²»æ–¹æ¡ˆï¼Œå‰‡æ‡‰åœ¨æ­¤å„²å­˜æ ¼é ‚éƒ¨é¸æ“‡æ¨¡å‹ã€‚\n",
        "#@markdown\n",
        "#@markdown æ‰¹æ¬¡å¤§å°è¶Šå¤§é€šå¸¸é€Ÿåº¦è¶Šå¿«ï¼Œä½†æœƒä½”ç”¨æ›´å¤šè¨˜æ†¶é«”ã€‚\n",
        "train_batch_size = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "#@markdown æˆ‘ç™¼ç¾ sdpa å’Œ xformers ä¹‹é–“æ²’æœ‰å¯¦è³ªå·®ç•°ã€‚\n",
        "cross_attention = \"sdpa\" #@param [\"sdpa\", \"xformers\"]\n",
        "#@markdown å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ A100ï¼Œå‰‡æ‡‰å•Ÿç”¨ bf16ã€‚\n",
        "mixed_precision = \"fp16\" #@param [\"bf16\", \"fp16\"]\n",
        "#@markdown å¿«å–æ½›åœ¨å¿«å–ç·©å­˜ï¼Œå°‡åœ¨æ¯å€‹åœ–åƒæ—é‚Šæ·»åŠ ä¸€å€‹ 250KB æ–‡ä»¶ï¼Œä½†æœƒä½¿ç”¨ç›¸ç•¶å°‘çš„è¨˜æ†¶é«”ã€‚\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "cache_latents_to_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown ä»¥ä¸‹é¸é …å°‡é—œé–‰ shuffle_tags ä¸¦åœç”¨æ–‡å­—ç·¨ç¢¼å™¨è¨“ç·´ã€‚\n",
        "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### â–¶ï¸ é€²éšè¨­å®š\n",
        "#@markdown å„ªåŒ–å™¨æ˜¯ç”¨æ–¼è¨“ç·´çš„æ¼”ç®—æ³•ã€‚ AdanW8Bit æ˜¯é è¨­è¨­å®šä¸¦ä¸”æ•ˆæœå¾ˆå¥½ï¼Œè€Œ Prodigy æœƒè‡ªå‹•ç®¡ç†å­¸ç¿’ç‡ï¼Œä¸¦ä¸”å¯èƒ½å…·æœ‰å¤šç¨®å„ªå‹¢ï¼Œä¾‹å¦‚ç”±æ–¼éœ€è¦æ›´å°‘çš„æ­¥é©Ÿè€Œè¨“ç·´é€Ÿåº¦æ›´å¿«ï¼Œä»¥åŠå°æ–¼å°å‹è³‡æ–™é›†æ›´å¥½åœ°å·¥ä½œã€‚\n",
        "optimizer = \"AdamW8bit\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\"]\n",
        "#@markdown AdamW8bit æ¨è–¦ä½¿ç”¨: `weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
        "#@markdown Prodigy æ¨è–¦ä½¿ç”¨: `decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`\n",
        "optimizer_args = \"weight_decay=0.1 betas=[0.9,0.99]\" #@param {type:\"string\"}\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
        "#@markdown å¦‚æœé¸æ“‡äº† Dadapt æˆ– Prodigy ä¸¦å‹¾é¸æ­¤æ¡†ï¼Œå‰‡ä»¥ä¸‹å»ºè­°å€¼å°‡è¦†è“‹ä»»ä½•å…ˆå‰çš„è¨­å®šï¼š<p>\n",
        "#@markdown `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`\n",
        "recommended_values_for_prodigy = True #@param {type:\"boolean\"}\n",
        "\n",
        "if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
        "  if recommended_values_for_prodigy:\n",
        "    unet_lr = 0.75\n",
        "    text_encoder_lr = 0.75\n",
        "    network_alpha = network_dim\n",
        "\n",
        "#@markdown ä½ å¯ä»¥åœ¨é€™è£¡å¯«ä¸‹ä½ çš„ Google Drive ä¸­çš„è·¯å¾‘ï¼Œä»¥è¼‰å…¥ç¾æœ‰çš„ Lora æª”æ¡ˆï¼Œä»¥ç¹¼çºŒè¨“ç·´ã€‚\n",
        "#@markdown **è­¦å‘Š** é€™ä¸æ˜¯ä¸€å€‹é•·æ™‚é–“çš„è¨“ç·´éšæ®µã€‚æ¯å€‹ epoch éƒ½æ˜¯å¾é ­é–‹å§‹ï¼Œä¸¦ä¸”å¯èƒ½æœƒæœ‰æ›´å·®çš„çµæœã€‚\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "if continue_from_lora:\n",
        "  if not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "    continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"ğŸ“‚ é€£æ¥åˆ° Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  if not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"ğŸ’¥ éŒ¯èª¤ï¼šç„¡æ³•è®€å–æ—¢æœ‰ Lora æª”æ¡ˆï¼Œè¨“ç·´å°‡ç„¡æ³•å¾æ­¤ç¾æœ‰ Lora ç¹¼çºŒè¨“ç·´ã€‚\")\n",
        "\n",
        "#@markdown ### â–¶ï¸ æº–å‚™å¥½äº†\n",
        "#@markdown ä½ ç¾åœ¨å¯ä»¥åŸ·è¡Œæ­¤å„²å­˜æ ¼ä¾†é–‹å§‹è¨“ç·´ã€‚ç¥ä½ å¥½é‹ï¼<p>\n",
        "\n",
        "\n",
        "# ğŸ‘©â€ğŸ’» Cool code goes here\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_xl_wrapper.py -q -O train_network_xl_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install torch==2.2.1+cu121 accelerate==0.19.0 transformers==4.30.2 diffusers==0.18.2 bitsandbytes==0.40.0.post4 opencv-python==4.7.0.68 jax==0.4.23 jaxlib==0.4.23\n",
        "  !pip install pytorch-lightning==1.9.0 voluptuous==0.13.1 toml==0.10.2 ftfy==6.1.1 einops==0.6.0 safetensors pygments\n",
        "  !pip install huggingface-hub==0.20.3 invisible-watermark==0.2.0 open-clip-torch==2.20.0 dadaptation==3.1 prodigyopt==1.0 lion-pytorch==0.1.2\n",
        "  !pip install -e .\n",
        "  if cross_attention == \"xformers\":\n",
        "    !pip install -q xformers==0.0.26.dev778\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if LOWRAM:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nğŸ’¿ æª¢æŸ¥è³‡æ–™é›†...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"ğŸ’¥ éŒ¯èª¤ï¼šè«‹é¸æ“‡æ­£ç¢ºçš„å°ˆæ¡ˆåç¨±ã€‚\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datconf[\"datasets\"][0][\"subsets\"] if not (d[\"is_reg\"] and d[\"is_reg\"] == True)}\n",
        "    except:\n",
        "      print(f\"ğŸ’¥ éŒ¯èª¤ï¼šä½ çš„è‡ªè¨‚è³‡æ–™é›†çµæ§‹éŒ¯èª¤ï¼Œè«‹ç¢ºèªè³‡æ–™é›†çµæ§‹ã€‚\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"ğŸ’¥ éŒ¯èª¤ï¼šè³‡æ–™å¤¾ {folder.replace('/content/drive/', '')} ä¸å­˜åœ¨ã€‚\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"ğŸ’¥ éŒ¯èª¤ï¼šä½ çš„ {folder.replace('/content/drive/', '')} è³‡æ–™å¤¾æ²’æœ‰è³‡æ–™ã€‚\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((\".txt\", \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"ğŸ’¥ éŒ¯èª¤ï¼šéŒ¯èª¤çš„è³‡æ–™é›†çµæ§‹ \\\"{f}\\\"ï¼Œä¸­æ–·åŸ·è¡Œã€‚\")\n",
        "      return\n",
        "\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"ğŸ’¥ éŒ¯èª¤ï¼šéŒ¯èª¤çš„è·¯å¾‘æˆ– Lora ä¸å­˜åœ¨ã€‚ç¯„ä¾‹ï¼š/content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"ğŸ“\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"ğŸ“ˆ è®€å– {img} åœ–ç‰‡ï¼Œé‡è¤‡ {rep} æ¬¡ï¼Œå…±æœ‰ {img*rep} å€‹æ­¥é©Ÿã€‚\")\n",
        "  print(f\"ğŸ“‰ è¨“ç·´æ‰¹æ¬¡ {train_batch_size} é™¤ä»¥ç¸½æ­¥é©Ÿæ•¸é‡ {pre_steps_per_epoch}ï¼Œå¾—åˆ°æ¯ä¸€è¼ªæ¬¡è™•ç† {int(steps_per_epoch)} å€‹æ­¥é©Ÿã€‚\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"ğŸ”® æœ€å¤§è¼ªæ¬¡ {max_train_epochs}ï¼Œå¤§ç´„æœƒæœ‰ {total_steps} å€‹ç¸½è¨“ç·´æ­¥é©Ÿã€‚\")\n",
        "  else:\n",
        "    print(f\"ğŸ”® ç¸½è¨“ç·´æ­¥é©Ÿ {total_steps}ï¼Œä¸¦é™¤ä»¥ {estimated_epochs} å€‹è¼ªæ¬¡ã€‚\")\n",
        "\n",
        "  if total_steps > 12000:\n",
        "    print(\"ğŸ’¥ éŒ¯èª¤ï¼šä½ çš„ç¸½è¨“ç·´æ­¥é©Ÿéé«˜ï¼Œå¯èƒ½æœƒé€ æˆéŒ¯èª¤ã€‚ä¸­æ–·è¨“ç·´ã€‚\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\nâ­• ä½¿ç”¨è‡ªè¨‚è¨­å®šæª” {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"vae\": vae_file,\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": cross_attention == \"xformers\",\n",
        "        \"sdpa\": cross_attention == \"sdpa\",\n",
        "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma > 0 else None,\n",
        "        \"lowram\": LOWRAM,\n",
        "        \"no_half_vae\": True,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"full_bf16\": mixed_precision == \"bf16\",\n",
        "        \"cache_latents\": cache_latents,\n",
        "        \"cache_latents_to_disk\": cache_latents_to_drive,\n",
        "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
        "        \"min_timestep\": 0,\n",
        "        \"max_timestep\": 1000,\n",
        "        \"prior_loss_weight\": prior_loss_weight,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"output_name\": project_name,\n",
        "        \"output_dir\": output_folder,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"logging_dir\": log_folder,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nğŸ“„ Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"â­• Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_no_upscale\": bucket_no_upscale,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"min_bucket_reso\": 256,\n",
        "        \"max_bucket_reso\": 4096,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"\\nğŸ“„ è¨­å®šæª”å·²å„²å­˜åˆ° {config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file, vae_url, vae_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if load_diffusers:\n",
        "    if 'huggingface.co' in real_model_url:\n",
        "        match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', real_model_url)\n",
        "        if match:\n",
        "            username = match.group(1)\n",
        "            model_name = match.group(2)\n",
        "            model_file = f\"{username}/{model_name}\"\n",
        "            fs = HfFileSystem()\n",
        "            existing_folders = set(fs.ls(model_file, detail=False))\n",
        "            necessary_folders = [ \"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\" ]\n",
        "            if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n",
        "              print(\"ğŸƒ Diffusers model identified.\")\n",
        "              return True\n",
        "    print(\"ğŸƒ ç„¡æ³•è®€å–æ¨¡å‹æª”æ¡ˆã€‚\")\n",
        "\n",
        "  if not model_file:\n",
        "    if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "      model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "    else:\n",
        "      model_file = \"/content/downloaded_model.safetensors\"\n",
        "      if os.path.exists(model_file):\n",
        "        !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", model_url):\n",
        "    if m.group(2):\n",
        "      model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "    if m := re.search(r\"modelVersionId=([0-9]+)\", model_url):\n",
        "      real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    else:\n",
        "      raise ValueError(\"optional_custom_training_model_url contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if not os.path.exists(vae_file):\n",
        "    !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{vae_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"é‡æ–°å‘½åç‚º {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"ğŸ“‚ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nğŸ­ å®‰è£ç›¸ä¾å¥—ä»¶...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\nâœ… å®‰è£å®Œæˆï¼Œè€—æ™‚ {int(t1-t0)} ç§’ã€‚\")\n",
        "  else:\n",
        "    print(\"\\nâœ… ç›¸ä¾å¥—ä»¶å·²å®‰è£ã€‚\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nğŸ”„ ä¸‹è¼‰æ¨¡å‹...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nğŸ’¥ éŒ¯èª¤ï¼šæ‚¨é¸æ“‡çš„æ¨¡å‹ç„¡æ•ˆæˆ–å·²æå£ï¼Œæˆ–ç„¡æ³•ä¸‹è¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ civitai æˆ– Huggingface éˆæ¥ï¼Œæˆ–ä»»ä½•ç›´æ¥ä¸‹è¼‰éˆæ¥ã€‚\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nğŸ”„ æ¨¡å‹å·²ä¸‹è¼‰ã€‚\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\nâ­ å•Ÿå‹•è¨“ç·´...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --quiet --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_xl_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### âœ… å®Œæˆ [å¾ Google é›²ç«¯ç¡¬ç¢Ÿä¸‹è¼‰æ‚¨çš„ Lora](https://drive.google.com/drive/my-drive)\\n\"\n",
        "                     \"### æœƒæœ‰å¹¾å€‹æ–‡ä»¶ï¼Œä½ æ‡‰è©²å˜—è©¦æœ€æ–°ç‰ˆæœ¬ï¼ˆæ—é‚Šæ•¸å­—æœ€å¤§çš„æ–‡ä»¶ï¼‰\"))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      },
      "source": [
        "## *ï¸âƒ£ æ“´å……åŠŸèƒ½\n",
        "\n",
        "åœ¨é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œä½ å¯ä»¥åŸ·è¡Œä»¥ä¸‹çš„åŠŸèƒ½ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4916Eu1tb9"
      },
      "source": [
        "### ğŸ“š è¤‡æ•¸è³‡æ–™å¤¾çš„è³‡æ–™é›†\n",
        "ä»¥ä¸‹çš„æ¨£ç‰ˆå…è¨±ä½ åœ¨è³‡æ–™é›†ä¸­ï¼Œå®šç¾©è¤‡æ•¸çš„è³‡æ–™å¤¾ã€‚ä½ éœ€è¦åŒ…å«æ¯å€‹è³‡æ–™é›†çš„æª”æ¡ˆè·¯å¾‘ï¼Œä¸”æŒ‡å®šæ¯å€‹è³‡æ–™é›†çš„é‡è¤‡æ¬¡æ•¸ã€‚ä½ å¯ä»¥ç›´æ¥è¤‡è£½ `[[datasets.subsets]]` å€å¡Šï¼Œç°¡å–®çš„å¢åŠ ä½ çš„è³‡æ–™é›†ã€‚\n",
        "\n",
        "ç•¶ä½ ä½¿ç”¨é€™å€‹è¨­å®šï¼Œåœ¨åŸæœ¬è¨“ç·´ä¸­çš„é‡è¤‡æ¬¡æ•¸çš„è¨­å®šå°‡æœƒè¢«å¿½ç•¥ï¼Œä¸”ä¾æ“šå°ˆæ¡ˆçš„è³‡æ–™é›†è¨­å®šä¹Ÿæœƒè¢«å¿½ç•¥ã€‚\n",
        "\n",
        "ä½ å¯ä»¥åŠ å…¥ `Ã¬s_reg = true` å°‡æŸä¸€å€‹è³‡æ–™é›†è¨­å®šç‚ºæ­£è¦åŒ–ï¼ˆregularizationï¼‰è³‡æ–™ã€‚\n",
        "ä½ ä¹Ÿå¯ä»¥è¨­å®šå„ç¨®ä¸åŒçš„åƒæ•¸ï¼Œä¾‹å¦‚ `keep_tokens`, `flip_aug` ç­‰ç­‰ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 804,
          "status": "ok",
          "timestamp": 1708481414906,
          "user": {
            "displayName": "Samuel",
            "userId": "11002898838993959229"
          },
          "user_tz": 180
        },
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/good_images\"\n",
        "num_repeats = 3\n",
        "is_reg = false\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/normal_images\"\n",
        "num_repeats = 1\n",
        "is_reg = false\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/example/dataset/reg_images\"\n",
        "num_repeats = 1\n",
        "is_reg = true\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WDjkp4scvPgE"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ“‚ Unzip dataset\n",
        "#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\n",
        "zip = \"/content/drive/MyDrive/my_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"ğŸ“‚ Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"âœ… å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKWlpsG0jrX3"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ”¢ Count datasets\n",
        "#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\n",
        "folder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ“‚ é€£æ¥åˆ° Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"ğŸ“{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcTjh07x90Ro"
      },
      "source": [
        "# ğŸ“ˆ ç¹ªè£½è¨“ç·´çµæœ\n",
        "æ‚¨å¯ä»¥åœ¨é‹è¡Œè¨“ç·´å™¨å¾ŒåŸ·è¡Œæ­¤æ“ä½œã€‚é™¤éæ‚¨çŸ¥é“è‡ªå·±åœ¨åšä»€éº¼ï¼Œå¦å‰‡æ‚¨ä¸éœ€è¦é€™å€‹ã€‚\n",
        "ä¸‹é¢çš„ç¬¬ä¸€å€‹å„²å­˜æ ¼å¯èƒ½ç„¡æ³•è¼‰å…¥æ‚¨çš„æ‰€æœ‰æ—¥èªŒã€‚ç¹¼çºŒå˜—è©¦ç¬¬äºŒå€‹å–®å…ƒæ ¼ï¼Œç›´åˆ°åŠ è¼‰æ‰€æœ‰è³‡æ–™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TRI3eX90Rp"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rM5SLq990Rp"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
